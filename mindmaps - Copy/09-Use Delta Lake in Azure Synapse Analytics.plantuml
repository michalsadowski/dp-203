@startmindmap
<style>
node {
    Margin 30
}
:depth(0) {
    BackgroundColor #091f2c
    FontColor #ffffff
    LineThickness 3.0
}
.topic1 {
    BackgroundColor #ffb900
    LineColor #ffb900
    LineThickness 3.0
    FontColor #000000
}
.topic2 {
    BackgroundColor #f4364c
    LineColor #f4364c
    LineThickness 3.0
    FontColor #ffffff
}
.topic3 {
    BackgroundColor #c5b4e3
    LineColor #c5b4e3
    LineThickness 3.0
    FontColor #000000
}
.topic4 {
    BackgroundColor #e1d3c7
    LineColor #e1d3c7
    LineThickness 3.0
    FontColor #000000
}
.topic5 {
    BackgroundColor #07641d
    LineColor #07641d
    LineThickness 3.0
    FontColor #ffffff
}
</style>
+ Delta Lake
++ 1) Understand Delta Lake <<topic1>>
+++ What is it? <<topic1>>
++++_ Open-source storage layer that adds relational database semantics to Spark
++++_ Serialization format
+++ Benefits <<topic1>>
++++_ CRUD
++++_ Support for ACID transactions (Atomicity, Consistency, Isolation, Durability)
++++_ Data versioning and time travel
++++_ Support for batch and streaming workloads
++++_ Standard formats and interoperability
++ 2) Create Delta Lake tables <<topic2>>
+++_ Creating a Delta Lake table from a dataframe
+++_ Making conditional updates (update, delete, and merge operations)
+++_ Querying a previous version of a table
++ 3) Create catalog tables <<topic3>>
+++ External vs managed tables <<topic3>>
++++_ A managed table is defined without a specified location, \ndropping the table deletes the files
++++_ An external table is defined for a custom file location, \ndropping the table does not delete the files
+++ How to create <<topic3>>
++++_ Creating a catalog table from a dataframe (df.write.format("delta"))
++++_ Creating a catalog table using SQL (CREATE TABLE MyExternalTable USING DELTA LOCATION '/delta/mytable')
++++_ Using the DeltaTableBuilder API
++ 4) Use Delta Lake with streaming data <<topic4>>
+++ Spark Structured Streaming <<topic4>>
+++ Streaming with Delta Lake tables <<topic4>>
++++_ As a streaming source \n(spark.readStream.format("delta"))
++++_ As a streaming sink \n(stream_df.writeStream.format("delta").option("checkpointLocation", "").start())
++ 5) Use Delta Lake in a SQL pool <<topic5>>
+++_ Querying delta formatted files with OPENROWSET
+++_ Querying catalog tables
@endmindmap